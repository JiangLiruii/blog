---
layout:       post
title:        "WPO(Web Performance Optimization) 中"
subtitle:     "无线通信"
date:         2018-12-15 12:00:00
author:       "Lorry"
header-mask:  0.3
header-img:   '/img/Fast-Moving-Light-Wallpaper.jpg'
catalog:      true
multilingual: false
tags:
    - web
---

现在无线连接无处不在, 用户也不会关心你的应用是不是"有线应用", 他们只希望不管在什么情况下, 访问相同的网址, 有相同的使用体验, 得到相同的结果, 甚至更好.

无线标准有相当多种:
WIFI, Bluetooth, MIMAX, ZigBee, NFC, LTE, HSPA, EV-DO, 甚至更早的3G, 卫星通信.如果要把每一个都搞懂然后去优化那是需要相当大的勇气, 好消息的大多数无线技术都是基于通用的规则, 相同的权衡因素, 性能指标以及约束条件, 只要把影响无线性能的基本原理搞清楚, 其他问题就迎刃而解了.

# 无线网络的类型
## 按照影响地理范围划分

|类型|范围|应用|标准|
|---|---|---|---|
|个人局域网(PAN Personal area network)|个人活动范围|替代有线传输|蓝牙, ZigBee, NFC|
|本地局域网(LAN, Local area network)|一栋建筑或一个校园|有线网络的无线扩展|IEEE 802.11(W-Fi)|
|城域网(MAN, Metropolitan area netw)|城市范围|无线内联网|IEEE 802.15(WiMAX)|
|广域网(WAN, Wide area network)|全球范围|无线网络|蜂窝(UMTS, LTE等)|

# 无线网络性能基础

每种类型的无线技术都有其自由的一套约束和限制, 但是无论哪种无线技术, 都有一个最大通道容量. 信道容量是最大的信息速率

![](/img/channel_capacity.svg)

- C 是信道荣祥, 单位为bits/s
- BW是可用带块, 单位为hz
- S是信号, N是噪声, 单位为W

## 带宽 BW
不像有线网络通过线缆将各个节点连接起来, 无线电是一种公用的机制 - **radio-wave** 无线电波, 更准确的将是电磁辐射. 收发双方都必须遵循一个使用频率. 比如802.11b 和 802.11g标准都是使用的2.4-2.5GHz频带.

 在美国, 这个过程是由FCC(Federal Communications Commission), 不同政府由不同规则, 所以有可能在不同国家由不一样的频率范围

![](/img/FCC_Frequency_Band.svg)

除开政治原因, 共享相同的频带能保证兼容的交互, 性能因素最重要的就是分配的频带范围. 在上述公式中信道总体比特率与分配的带宽成正比. 这就是最初802.11n相比于早期WIFI标准提升性能的方式.

最后值得注意的是不是所有的频段提供相同的性能, 越低的频率信号传输得越远, 并且覆盖更广的面积(macrocells-大蜂窝), 但是需要花费更多的大型天线还要由更多的竞争(接入的范围大). 

高频信号可以传输更多的数据, 但是传不远, 从而形成较小的覆盖范围(microcells-小蜂窝)并且需要更多的基础设施.

不同的频段对不同的应用来说价值也不一样, 对广播来说, 更需要低频, 而双向通信则需要高频, 提供更大的带宽, 竞争也会少一些.

### 分频历史

这个比较有意思, 提出来说一下, 在1912年前, **任何人**都可以使用她想要的任何频段. 1912年, 美国通过了射频法案, 规范射频光谱的使用, 而设立该法案的初心是因为泰坦尼克的沉没--如果有专用的紧急信道, 其他更多的船只就可以收到信号前去营救, 这样就不会死那么多人了.1934年FCC出台通信法案并负责分配频谱.

1947年, **ISM(industrial, scientific, medical)频带**是第一个被国际通信委员会建立并在各国有保留频段. 2.4-2.5GHz(100MHz) 和 5.725-5.825GHz(159MHz)带宽, 这也是现在用的无线通信(WiFi)的频段之一. 这些频段也被视为"没有证书的频段", 可以允许任何人允许在无线网上, 不管是因为商业还是私有用途. 在这些频段和使用的硬件遵从具体的技术标准,比如 transmit power, 传输功率

最后, 因为不断增长的无线通信需求, 政府开始举办频谱拍卖, 售出证书以允许在特定频段上传输信号. 2008年, 698-806MHz在美国拍出了**196亿**美金.足以证明带宽的稀有和珍贵. 这将继续成为一个高竞争的领域.

### 信号功率
除开带宽之外, 次要因素就是收端和发端的信号功率大小. 也成为信噪比( SNR S/N ratio), 值越大,信号就越强.
通常来说所有的无线信号是共用媒质的, 比如空气, 这也就意味着会被其他设备产生信号干涉, 举例:
微波炉工作在2.5GHz, 笔记本也用这个频段使用 WiFi, 就会发生跨标准感谢. 如果手机其他 WiFi 设备, 比如邻居家的WiFi,甚至连接同一个 WiFi 的同事的电脑, 也能对你的传输产生干涉.

最好的情况是, 你自己用自己的某个特殊评断, 没有任何噪声和干扰, 但那几乎是不可能的. 只能保证需要的数据在即使有干扰的情况下也能被收到, 可以尝试
- 尽可能提高传输功率, 增大 signal 的值
- 降低传输距离

还有一个因素是路径损耗, 距离越长, 信号量越少的现象. 这个现象没法避免, 具体可以参见 [维基百科](https://en.wikipedia.org/wiki/Path_loss)

![](/img/Optimize/SNR.svg)

上图展示了两个重要的问题

1. 远近问题
    1. 接受者获取强信号的条件
    2. 接受者不可能从比1条件更弱的信号中获取信息有效的区分出信息
2. 蜂窝呼吸
    1. 覆盖范围的条件, 或者说信号传递的距离
    2. 会基于噪音和干扰强度不停地扩张或收缩

### 数模转换

可用的带宽和信噪比是影响无线信道容量的主要的两个物理因素. 但是信号量算法的编码也能有重大的影响. 这里涉及到数模转换 - 把数字信号的0和1放到模拟信号(射频信号)上.

- 收发两端可以处理1000个脉冲每秒(1000 baud)--波特率
- 每个传输元代表不同的位序列, 由选择的字母表决定比如 2-bit 字母表为: 00, 01, 10, 11
- 通道的比特率是1000 baud * 2bits = 2000bits/s

数模转换的算法取决于可用的技术, 收发双端的计算功率和信噪比. 越高等级的转换表可以降低干扰, 但是也会提高成本.

### 衡量现实世界的无线性能

所有的射频通信基于:
- 共享通信媒质
- 受限与使用具体的带宽频率范围
- 受限于使用具体的传输功率比
- 受限于不停变化的背景噪音和干扰
- 受限于无线技术类型
- 受限于设备, 成型因素( form factor), 功率等

所有的无线技术都有建议的峰值, 或数据的最大值. 举例: 802.11g 标准是 54Mbit/s, 802.11n 标准是600Mbit/s, 类似的, 一些移动运营商选出 LTE 的吞吐量可达100+Mbit/s, 但是, 这些都是在最理想的情况下的数据. 啥事最理想的条件?
- 分配的带宽的最大值
- 专享带宽
- 最小化甚至没有背景噪音
- 最高的数模转换吞吐量
- 多个射频流( MIMO )

所以这些因素就可以影响无线网络性能了:
- 收发端举例
- 当前位置的背景噪声
- 相同网络下不同用户干扰
- 附近不同网络干扰
- 收发双端的可用传输性能
- 处理性能和选择的模数转换类型

上述的影响性能因素只考虑了吞吐量, 还没有考虑延迟, 因不同的无线方案而不同, 之后会详细说明延迟的

## WiFi
- 简单,便宜, 无须许可,可在任何地方由任何人部署, 是部署最广也最受欢迎的无线标准.
- 最初来自WiFi联盟推广无线本地局域网技术( wireless LAN )
- 严格来说WiFi 设备需要得到 WiFi 联盟的检测证书, 并带有 WiFi 的名字和 logo, 实际上现在基于 IEEE 802.11标准的都叫 WiFi
- 1997年提出802.11, 但多少是作为802.3的改良和扩展,直到1999年802.11b 标准才被引入.
- 2.4GHz ISM 带宽

### 从以太网到无WLAN
以太网基于802.3, WLAN 基于802.11

![](/img/Optimize/8023_and_80211.svg)

[ALOHAnet](https://zh.wikipedia.org/wiki/ALOHAnet)是1971年6月诞生的第一个无线网络系统. 它成为以太网和WiFi 的基础, 他是基于随机存取信道, 意味着没有中央处理, 没有规划者来控制哪些设备在什么时候允许传输, 而是所有的设备都在他们自己的信道上, 并且所有设备协同一起工作来保证共享信道的性能.

以太网曾经还出过一个叫 CSMA 的协议 (carrier sense multiple access):
1. 检查有没有人在传输
2. 如果信道忙, 等待释放
3. 一旦释放后立即开始传输

显而易见很容易产生冲突, 于是有了 CSMA/CD(collision detection):
- 如果一个冲突被检测到, 所有的传播都立即停止
- 休眠一个随机间隔(线性增加的)
- 多个冲突者不会有相同的休眠间隔, 也就不会在重启时再有冲突

WiFi 的模型跟上述有些相似, 但是因为硬件限制, 无法再数据发送是发现冲突, 所以 WiFi 依赖于冲突避免( CSMA/CA collision avoidance).每个发送方都会通过只在信道为空闲的时候发死欧诺个来避免冲突, 然后发送他的一整个消息帧. 一旦 WiFi 帧被发送, 发送方须等待一个接收方可以处理下一个传输的告知. 这里信道使用率很重要, 尽量保持在10%以下. 随着使用率的增高, 冲突的数量也会很快上升.

### WiFi 标准和特性

|802.11 protocol|Release|Freq(GHz)|Bandwidth(MHz)|单个流的速率(Mbit/s|最大多输入多输出流数|
|---|---|---|---|---|---|---|
|b|	Sep 1999|	2.4|	20|	1, 2, 5.5, 11|	1|
|g|	Jun 2003|	2.4|	20|	6, 9, 12, 18, 24, 36, 48, 54|	1|
|n|	Oct 2009|	2.4|	20|	7.2, 14.4, 21.7, 28.9, 43.3, 57.8, 65, 72.2|	4|
|n|	Oct 2009|	5|	40|	15, 30, 45, 60, 90, 120, 135, 150|	4|
|ac|	~2014|	5|	20, 40, 80, 160|	up to 866.7|	8|

现如今 b 和 g 是部署和支持最广的标准
- 利用无证书的2.4GHz ISM 频带
- 使用20MHz 带宽
- 最多一个数据流
- 传输功率最大200mW

如何优化 WiFi 性能? 把 n 升级成 ac, 这样每个信道带宽从20到40MHz, 使用更高的数模转换, 添加了更多的射频来平行传输多个流( MIMO ), 因此 ac 可以由 G+ 级别的吞吐量

### WiFi 性能检测和优化

WiFi 多媒体( WiFi Multimedia)提出在对延迟敏感的应用中(比如视频, 音频等)QoS(Quality of Service)的概念, 但是只有一些路由, 甚至更少数的部署客户端可以感知到. 同时, 所有的流量不管是你自由网络还是附近的 WiFi 都必须竞争者使用射频资源.

路由器可以为客户端设置一些 QoS 的策略, 比如最大化每个客户端的速率或者根据流量限速. 但是不能控制其他人产生的流量. WiFi 的易用性促进了使用它的网络无处不在,也同时导致了许多性能问题. 实践上来说, 在高密度的城市或者办公室环境中发现好几十个不同的重叠 WiFi 网络是很常见的.

![](/img/Optimize/overlapping.png)

最常用的2.4GHz 频带提供了三个不重叠的20MHz 信道, 1, 6, 11.但是在不同的国家也是不一样的, 有些可以允许使用13,14信道, 而另一些则被限制在较小的子集中.即便不看区域因素, 只要附近有超过2个或3个的 WiFi, 就会有重叠和竞争.

![](/img/Optimize/2_4GHZ_band.svg)

802.11g 的设备和路由器可以上到54Mbps, 但是这时候一个使用相同 WiFi 信道的邻居, 开始通过 WiFi 下载高清 电影, 带宽就会减少一半, 甚至更多. 接入点对此没有任何办法, 这是特性, 不是 bug.

不幸的是, 延迟性能也没有更好, 没有任何可以保证在客户端和 WiFi 接入点的第一跳的延迟. 因为你在同其他无线使用者竞争进入一个共享的信道. 好消息是, 如果你是一个爱尝鲜的人, 可以试试使用802.11n 和802.11ac 标准的5GHz, 提供了一个更宽的频率范围, 而且现在在大多数环境中用得还比较少. 一些双频段路由器可同时支持2.5GHz 和5GHz.下表是典型的2.4GHz 和5GHz 的无线网首跳延迟

|Freq (GHz)|	Median (ms)	|95% (ms)	|99% (ms)|
|---|---|---|---|
|2.4	|6.22	|34.87	|58.91|
|5	|0.90	|1.58	|7.89|

所以, 综上述:
- WiFi 没有任何带宽或延迟保证
- WiFi 基于环境信噪比提供动态带宽
- 传输功率限制在200mW 以内.
- WiFi 接入点覆盖被设计时分配的信道(2.4,还是5)
- WiFi 接入点之间对同一个射频信道进行竞争.

没有什么"典型"值, 这个范围会根据标准, 地理位置, 使用设备, 当地射频环境而变化, 如果很幸运整个 WiFi 只有一个使用者, 那么可以由一个很高的吞吐量, 低延时, 低波动等, 一旦一开始竞争, 那所有的都会有竞争.

因为没有带宽保证, 所以会有不同的信息流去适配不同的带宽改变, 比如带宽好的时候传输 1080p 的数据流, 带宽不好的时候传输720或480p 的数据流. **自适应比特流**

|Container|	Video resolution|	Encoding|	Video bitrate (Mbit/s)|
|---|---|---|---|
|mp4	|360p|	H.264|	0.5|
|mp4	|480p|	H.264|	1–1.5|
|mp4	|720p|	H.264|	2–2.9|
|mp4	|1080p|	H.264|	3–4.3|

因为没有延迟的保证, 如果是对延迟敏感的就不要使用 WiFi 进行传输了, 这也是使用不可靠 UDP 传输的WebRTC 诞生的一个诱因. 当然改变传输并没有影响射频网络, 但是能减低协议和应用层面的延迟.

#### WiFi 的丢包

看到这些竞争导致的不稳定, 是否会让 WiFi 传输的包丢失很多呢? 实际上不必过多担心 TCP 丢包, 因为数据链路和物理链路层会有自己的重发和错误校验机制, 而无线的冲突在网络七层结构中华是更高的层.换句话说, TCP 的丢包不是 WiFi 应该绝对考虑的问题, TCP 传输的绝对速率不会比大多数有线网络高, 反而相对于直接的 TCP 丢包, 更容易在低层的连接和物理层看到因为波动性导致包到达时间不同.

### WiFi 网络的优化

#### 不由带宽影响的因素
在实践中, WiFi 网络经常被视为有线 LAN 的扩展, 有线 LAN是通过 DSL, 线缆或光线连接到广域网. 2015年底, 在美国的平均用户来说, 传输速率在12.6Mbps, 而全球来说是5.1Mbps, 所以首先因素在于 WAN 带宽, 而不是 WiFi 本身.
|Rank|	Country	|Average Mbps	|Year-over-year change|
|---|---|---|---|
|-	|Global	|5.1	|14%|
1	|South Korea|	20.5	|-19%|
|2	|Sweden	|17.4	|23%|
|3	|Norway	|16.4	|44%|
|4	|Switzerland	|16.2	|12%|
|5	|Hong Kong	|15.8	|-2.7%|
|…	| 	 	 
|21	|United States	|12.6	|9.4%|

但是, 除开带宽瓶颈, 流量是在背连 WAN 连接当中不会考虑的因素(相当于有线连接), 特别是在大的数据容量和最大吞吐量的时候, 许多用户可能会在3G 或4G 会因为费用和带宽的因素非常敏感, 而这不是 WiFi 要考虑的.

这种排除假设法不是在所有环境都是正确的, 比如把 WiFi 建立在3G 或4G 的连接(移动热点), 但大多数情况下是. 结果下载, 更新, 数据流的使用都应该尽可能使用 WiFi.

## 移动网络
人口的增长带动无线设备的大量增长, 对移动网络的需求也只会越来越大.
### G的历史
|Generation	|Peak data |rate	|Description|
|---|---|---|---|
|1G	|no data|	Analog systems|
|2G	|Kbit/s|	First digital systems as overlays or parallel to analog systems|
|3G	|Mbit/s|	Dedicated digital networks deployed in parallel to analog systems|
|4G	|Gbit/s|	Digital and packet-only networks|

峰值光谱效率 bps/Hz, 它可以转换到4G的Gbit/s+ 峰值数据速率, 注意这里使用的都是峰值(peak),意味着所有的条件都是最理想情况下的数据

|Generation	|Data rate	|Latency|
|---|---|---|
|2G	|100–400 Kbit/s	|300–1000 ms|
|3G	|0.5–5 Mbit/s	|100–500 ms|
|4G	|1–50 Mbit/s|	< 100 ms|

### 第一个数据服务-2G
第一个商用1G网络诞生1979年的日本, 他是一个模拟系统没有提供任何数据容量, 1991年, 第一个2G网络诞生在芬兰, 它基于聚合GSM(Global System For Mobile Communications最初是Groupe Special Mobile)标准, 也是第一次把数字信号带入射频网络. 它开启了第一个电路切换移动数据服务, 比如SMS, 并且包传递效率峰值为9.6Kbit/s

90年代中期, GPRS(General Packet Radio Service)第一次引入GSM标准, 无线网的接入开始变得可用, 虽然仍然很慢(172Kbit/s), 典型的往返延时在好几百毫秒. GPRS和2G语音技术合起来被成为2.5G, 一些年一个, 网络被提升到使用EDGE(Enhanced Data rate for GSM Evolution) 峰值速率到了384Kbit/s, 第一个EDGE, 也被称为2.75G网络诞生于2003年的美国.

### 3GPP和3GPP2联盟

这些技术的实现没有一个具体的工厂标准, 比如最开始的GSM全球标准覆盖了80%到85%, 但是并不是只有一个, 还有高通自己的标准(IS-95)占据10%到15%, 主要在北美, 这就导致了全球漫游网络不可用的问题.下图是03到07年的移动标准市场

![](/img/Optimize/market_share_mobile.png)

#### 3GPP(3rd generation Partnership Project)
致力于在对GSM进行3G升级时发展全球移动通信网(Universal Mobile Telecommunication System--UMTS), 它被视为GSM的修正标准和新的LTE实验性标准

#### 3GPP2(3rd generation Partnership Project 2)
致力于发展基于CDMA2000技术的3G技术, 这是IS-96的高通标准

- 有两种主导性的部署在移动网络上的类型
- 3GPP和3GPP2 主导了每个技术的改良
- 3Gpp和3Gpp2标准不能让设备互通.

不存在孤立的3G或4G技术, 国际通信联盟(International Telecommunication Union--ITU)指定国际标准和性能指标, 3GPP和3GPP2使用各自的技术去定义这些标准, 达到实现指标甚至超过预期.

